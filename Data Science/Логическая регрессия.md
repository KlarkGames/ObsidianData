#Data_Science 

Основная суть логической регрессии заключается в определении [[Сигмоида|сигмоиды]]:
$$\sigma(x)=\frac{1}{1+e^{-x}}$$
Ограничения функции по оси ординат лежат на промежутке $[0, 1]$, а область определения есть вся числовая прямая.

Соответственно модель мы будем искать в следующем виде:
$$f(\mathbf{x}) = \sigma(\sum\limits_{i=0}^{n}\mathbf{w}_{i}\mathbf{x}_{i})$$
Для проблемы классификации берется следующая [[Функция невязки|функция ошибки]]:
$$ln(P(Y=y_{i}|\mathbf{x}_i))=y_{i}ln(f(\mathbf{x}_i))+(1-y_{i})ln(1-f(\mathbf{x}_{i}))$$
$$Loss(\mathbf{w}) = -\sum\limits_{i=1}^{l}ln(P(Y=y_{i}|\mathbf{x}_{i}))\rightarrow {min}_{w}$$
$$Loss(\mathbf{w}) = -\sum\limits_{i=1}^{l}y_{i}ln(f(\mathbf{x}_i))+(1-y_{i})ln(1-f(\mathbf{x}_{i}))$$
Дальше [[Минимизация функций нескольких переменных|минимизируем]] функцию ошибки наиболее удобным нам способом по весам $\mathbf{w}$.
