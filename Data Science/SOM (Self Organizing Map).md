#Data_Science #Machine_learning #Алгоритмы 

Самоорганизующиеся карты или нейронная сеть Кохонена есть модель, которая позволяет выделить известное количество групп среди входных значений. Используется для задач кластеризации и уменьшения размерности данных.

## Алгоритм работы

Для начала требуется определиться с тем, какого вида будет сетка: прямоугольная или гексагональная. Используемая в данной работе реализация НС предполагает использование прямоугольной сетки, хотя стоит отметить, что гексагональная вариация показывает более точно приближение, из-за возможности более точно определить расстояние между узлами.

Далее инициализируются координаты узлов. В данной вариации они инициализируются случайным образом.

Последующие шаги работы выполняются циклически, поэтому в описании будет фигурировать некий момент $t$ и предшествующий ему момент $t-1$. Итерации проходят по входящим данным. Они обозначены как $x_t$.

1. Первый шаг: определение ближайшего к $x_t$ узла $m_i$. Назовем его $m_c$. Для определения расстояния отлично подходит функция Евклидового расстояния. Таким образом ставится следующее условие $||m_c - x_t|| \leq ||m_i - x_t||$. Из точек, имеющих одинаковое расстояние от рассматриваемого образца, ближйшая выбирается случайным образом.
   
3. Вторым шагом является приближение точки "победителя" $m_c$ и ее ближайших соседей к рассматриваемому образцу $x_t$. Для этого используют 2 подхода: первый - гауссовскую функцию:
   $$h_{ci}(t) = \alpha(t)\cdot \exp{-\frac{{||r_c - r_i||}^2}{2\sigma^2(t)}}$$
   Или использовать более простой вариант: 
   $$h_{ci}(t) = \begin{cases} \alpha(t) & ||r_c - r_t|| \leq R \\ 0 & ||r_c - r_t|| > R\end{cases}$$
   Где $\sigma$ - монотонно убывающий с увеличением количества итераций сомножитель, $r_c$ и $r_i$ соответствующие растояния, $R$ - задаваемый аналитиком радиус, $\alpha(t)$ - learning rate.
   Дальше идет обновление координат точек следующим образом:
   $$m_i(t)=m_i(t-1)+h_{ci}(t)\cdot (x(t) - m_i(t-1))$$
   Т.е. все вектора узлов, являющимися соседями "победителя" приближаются к рассматриваемому наблюдению.

Таким образом итерационно узлы приближаются к своим кластерам, пока: а) не пройдет определенное количество итераций, б) изменения весов не станут слишком малы.