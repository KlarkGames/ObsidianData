#Data_Science 

Регуляризация модели это метод добавления к [[Функция невязки|функции невязки]] слагаемое, которое зависит от весов $w$. 

Выделяют $L_{1}$ и $L_{2}$ регуляризации:
$L_{2}$ регуляризация:
$$Loss(\mathbf{x}) = \sum\limits_{i=0}^{k}(\mathbf{x}_{i}^{T}\mathbf{w} - y_{i})^{2} + \beta \sum\limits_{j=0}^{k} \mathbf{w}_{i}^{2}$$
$L_{1}$ регуляризация:
$$Loss(\mathbf{x}) = \sum\limits_{i=0}^{k}(\mathbf{x}_{i}^{T}\mathbf{w} - y_{i})^{2} + \beta \sum\limits_{j=0}^{k} |\mathbf{w}_{i}|$$

Основная задача регуляризации это борьба с [[Переобучение модели|переобучением]] путем штрафования за излишне большие по модулю веса модели.

Регуляризация модели является частным случаем [[Решение задач условной минимизации#Метод штрафных функций|метода штрафных функций]].