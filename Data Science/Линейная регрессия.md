#Data_Science 

Линейная регрессия есть алгоритм предсказания некой непрерывной величины $\mathbf{Y} \in \mathbf{R}$ по известному набору параметров $\mathbf{X}$.

Линейная регрессия входит в множество [[Обучение с учителем|алгоритмов обучения с учителем]].

Основной смысл следующий: задается некоторая [[Функция невязки|функция ошибки]] $f_{\theta}:\ \mathbf{X} \to \mathbf{Y_{pred}}$ которую мы [[Минимизация функций нескольких переменных|минимизируем]] по $\theta$.

Функция ошибки высчитывается как сумма ошибки всех тестовых образцов, обычно это:
$$loss_{(\theta)}(Y_{predicted}, Y_{true})= (Y_{true} - Y_{predicted})^{2}$$
Таким образом искомые параметры $\theta$ определяются как:
$$\mathbf{\theta}_{best} = {argmin}_{\theta}\frac{1}{\text{dataset size}}\sum\limits_{i}loss(y_{true}^{i}, f_{\theta}(x^{i}))$$
Те. такие, при которых функция потерь будет минимальна.

Сама же функция $f_{\theta}$ часто принимает значение: $(\mathbf{\theta}, \{1, \mathbf{X}\})$
Или же: $$f_{\theta}(\mathbf{X}) = \theta_{0}\cdot1 + \theta_{1}\cdot x_{1} + \dots + \theta_{n}\cdot x_{n}$$ Стоит заметить, что к вектору $\mathbf{X}$ добавляется в начало элемент 1. Таким образом количество параметров $\theta$ на один больше количества признаков.

Так же в качестве признаков могут быть производные от уже существующих признаков. Прим. $b_{0}= x_{1}\cdot x_{2},\ b_{1} = \sqrt{x_{4}}$.
Увеличение количества признаков может вести к [[Переобучение модели|переобучению]] модели.

![[Pasted image 20220822155754.png]]


Минимизировать можно различными способами представленными в статье "[[Минимизация функций нескольких переменных]]".

