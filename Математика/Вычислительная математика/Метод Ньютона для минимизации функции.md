#Вычислительная_математика 

Допустим есть некоторая функция нескольких переменных $f(x_{0},\dots,x_{n})$. Требуется найти ее минимум.
Метод Ньютона позволяет итерационно найти минимум функции используя [[Матрица Гессе|матрицу Гессе]].

Матрица Гессе для заданной функции выглядит следующим образом:

$$
\mathbf{H}=\frac{\partial^{2}f}{\partial \mathbf{x}^{2}}=
\begin{pmatrix}
	\frac{\partial^{2}f}{\partial x_{0}\partial x_{0}}&\frac{\partial^{2}f}{\partial x_{0}\partial x_{1}}&\dots&\frac{\partial^{2}f}{\partial x_{0}\partial x_{m}}\\
	\frac{\partial^{2}f}{\partial x_{1}\partial x_{0}}&\frac{\partial^{2}f}{\partial x_{1}\partial x_{1}}&\dots&\frac{\partial^{2}f}{\partial x_{1}\partial x_{m}}\\
	\dots&\dots&\dots&\dots\\
	\frac{\partial^{2}f}{\partial x_{m}\partial x_{0}}&\frac{\partial^{2}f}{\partial x_{m}\partial x_{1}}&\dots&\frac{\partial^{2}f}{\partial x_{m}\partial x_{m}}
\end{pmatrix}
$$

Допустим мы находимся в некоторой точке $x_{n}$.  Тогда следующая точка, которая будет нас приближать к минимуму функции может быть вычислена следующим образом:

$$\mathbf{H}(\mathbf{x}_{n+1}-\mathbf{x}_{n})=-grad f(\mathbf{x}_{n})$$

Где $grad(\mathbf{x}_{n})$ - [[Градиент|градиент]] функции в точке $\mathbf{x}_{n}$

Для метода Ньютона очень важно хорошее приближение начальной точки к точке минимума. При плохо обусловленной функции метод Ньютона теряет в точности результата.